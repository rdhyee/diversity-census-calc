{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, reproduce some of the findings from [What Makes Houston the Next Great American City? | Travel | Smithsonian](http://www.smithsonianmag.com/travel/what-makes-houston-the-next-great-american-city-4870584/), specifically the calculation represented in\n",
    "\n",
    "![Alt text](http://thumbs.media.smithsonianmag.com//filer/Houston-diversity-3.jpg__600x0_q85_upscale.jpg \"Optional title\")\n",
    "\n",
    "whose caption is\n",
    "\n",
    "<blockquote>To assess the parity of the four major U.S. ethnic and racial groups, Rice University researchers used a scale called the Entropy Index. It ranges from 0 (a population has just one group) to 1 (all groups are equivalent). Edging New York for the most balanced diversity, Houston had an Entropy Index of 0.874 (orange bar).</blockquote>\n",
    "\n",
    "The research report by *Smithsonian Magazine* is\n",
    "[Houston Region Grows More Racially/Ethnically Diverse, With Small Declines in Segregation: A Joint Report Analyzing Census Data from 1990, 2000, and 2010](http://kinder.rice.edu/uploadedFiles/Urban_Research_Center/Media/Houston%20Region%20Grows%20More%20Ethnically%20Diverse%202-13.pdf) by the Kinder Institute for Urban Research & the Hobby Center for the Study of Texas.  \n",
    "\n",
    "In the report, you'll find the following quotes:\n",
    "\n",
    "<blockquote>How does Houston’s racial/ethnic diversity compare to the racial/ethnic\n",
    "diversity of other large metropolitan areas? The Houston metropolitan\n",
    "area is the most racially/ethnically diverse.</blockquote>\n",
    "\n",
    "....\n",
    "\n",
    "<blockquote>Houston is one of the most racially/ethnically diverse metropolitan\n",
    "areas in the nation as well. *It is the most diverse of the 10 largest\n",
    "U.S. metropolitan areas.* [emphasis mine] Unlike the other large metropolitan areas, all\n",
    "four major racial/ethnic groups have substantial representation in\n",
    "Houston with Latinos and Anglos occupying roughly equal shares of the\n",
    "population.</blockquote>\n",
    "\n",
    "....\n",
    "\n",
    "<blockquote>Houston has the highest entropy score of the 10 largest metropolitan\n",
    "areas, 0.874. New York is a close second with a score of 0.872.</blockquote>\n",
    "\n",
    "....\n",
    "\n",
    "Your task is:\n",
    "\n",
    "1. Tabulate all the metropolian/micropolitan statistical areas.  Remember that you have to group various entities that show up separately in the Census API but which belong to the same area.  You should find 942 metropolitan/micropolitan statistical areas in the 2010 Census.\n",
    "\n",
    "1. Calculate the normalized Shannon index (`entropy5`) using the categories of White, Black, Hispanic, Asian, and Other as outlined in the [Day_07_G_Calculating_Diversity notebook](http://nbviewer.ipython.org/github/rdhyee/working-open-data-2014/blob/master/notebooks/Day_07_G_Calculating_Diversity.ipynb#Converting-to-Racial-Dot-Map-Categories) \n",
    "\n",
    "1. Calculate the normalized Shannon index (`entropy4`) by not considering the Other category.  In other words, assume that the the total population is the sum of White, Black, Hispanic, and Asian.\n",
    "\n",
    "1. Figure out how exactly the entropy score was calculated in the report from Rice University. Since you'll find that the entropy score reported matches neither `entropy5` nor `entropy4`, you'll need to play around with the entropy calculation to figure how to use 4 categories to get the score for Houston to come out to \"0.874\" and that for NYC to be \"0.872\".  [I **think** I've done so and get 0.873618 and \n",
    "0.872729 respectively.]\n",
    "\n",
    "1. Add a calculation of the [Gini-Simpson diversity index](https://en.wikipedia.org/wiki/Diversity_index#Gini.E2.80.93Simpson_index) using the five categories of White, Black, Hispanic, Asian, and Other.\n",
    "\n",
    "1. Note where the Bay Area stands in terms of the diversity index.\n",
    "\n",
    "For bonus points:\n",
    "\n",
    "* make a bar chart in the style used in the Smithsonian Magazine\n",
    "\n",
    "Deliverable:\n",
    "\n",
    "1. You will need to upload your notebook to a gist and render the notebook in nbviewer and then enter the nbviewer URL (e.g., http://nbviewer.ipython.org/gist/rdhyee/60b6c0b0aad7fd531938)\n",
    "2. On bCourses, upload the CSV version of your `msas_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hispanic or Latino Origin and Racial Subcategories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.census.gov/developers/data/sf1.xml\n",
    "\n",
    "compare to http://www.census.gov/prod/cen2010/briefs/c2010br-02.pdf \n",
    "\n",
    "I think the P0050001 might be the key category\n",
    "\n",
    "* P0010001 = P0050001\n",
    "* P0050001 = P0050002 + P0050010\n",
    "\n",
    "P0050002 Not Hispanic or Latino (total) = \n",
    "\n",
    "* P0050003 Not Hispanic White only \n",
    "* P0050004 Not Hispanic Black only\n",
    "* P0050006 Not Hispanic Asian only\n",
    "* Not Hispanic Other (should also be P0050002 - (P0050003 + P0050004 + P0050006)\n",
    "     * P0050005 Not Hispanic: American Indian/ American Indian and Alaska Native alone\n",
    "     * P0050007 Not Hispanic: Native Hawaiian and Other Pacific Islander alone\n",
    "     * P0050008 Not Hispanic: Some Other Race alone\n",
    "     * P0050009 Not Hispanic: Two or More Races\n",
    "\n",
    "* P0050010 Hispanic or Latino\n",
    "  \n",
    "P0050010 = P0050011...P0050017\n",
    "\n",
    "From [Hispanic and Latino Americans (Wikipedia)](https://en.wikipedia.org/w/index.php?title=Hispanic_and_Latino_Americans&oldid=595018646):  \n",
    "\n",
    "<blockquote>While the two terms are sometimes used interchangeably, Hispanic is a narrower term which mostly refers to persons of Spanish speaking origin or ancestry, while Latino is more frequently used to refer more generally to anyone of Latin American origin or ancestry, including Brazilians.</blockquote>\n",
    "\n",
    "and\n",
    "\n",
    "<blockquote>The Census Bureau's 2010 census does provide a definition of the terms Latino or Hispanic and is as follows: “Hispanic or Latino” refers to a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Spanish culture or origin regardless of race. It allows respondents to self-define whether they were Latino or Hispanic and then identify their specific country or place of origin.[52] On its website, the Census Bureau defines \"Hispanic\" or \"Latino\" persons as being \"persons who trace their origin [to]... Spanish speaking Central and South America countries, and other Spanish cultures\".</blockquote>\n",
    "\n",
    "In the [Racial Dot Map](http://bit.ly/rdotmap): \"Whites are coded as blue; African-Americans, green; Asians, red; Hispanics, orange; and all other racial categories are coded as brown.\"  \n",
    "\n",
    "In this notebook, we will relate the Racial Dot Map 5-category scheme to the P005\\* variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab --no-import-all inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame, Series, Index\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import census\n",
    "import us\n",
    "\n",
    "import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The census documentation has example URLs but needs your API key to work.  In this notebook, we'll use the IPython notebook HTML display mechanism to help out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = census.Census(key=settings.CENSUS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generators for the various census geographic entities of interest\n",
    "\n",
    "def states(variables='NAME'):\n",
    "    geo={'for':'state:*'}\n",
    "    states_fips = set([state.fips for state in us.states.STATES])\n",
    "    # need to filter out non-states\n",
    "    for r in c.sf1.get(variables, geo=geo):\n",
    "        if r['state'] in states_fips:\n",
    "            yield r\n",
    "            \n",
    "def counties(variables='NAME'):\n",
    "    \"\"\"ask for all the states in one call\"\"\"\n",
    "    \n",
    "    # tabulate a set of fips codes for the states\n",
    "    states_fips = set([s.fips for s in us.states.STATES])\n",
    "    \n",
    "    geo={'for':'county:*',\n",
    "             'in':'state:*'}    \n",
    "    for county in c.sf1.get(variables, geo=geo):\n",
    "        # eliminate counties whose states aren't in a state or DC\n",
    "        if county['state'] in states_fips:\n",
    "            yield county\n",
    "        \n",
    "\n",
    "def counties2(variables='NAME'):\n",
    "    \"\"\"generator for all counties\"\"\"\n",
    "    \n",
    "    # since we can get all the counties in one call, \n",
    "    # this function is for demonstrating the use of walking through \n",
    "    # the states to get at the counties\n",
    "\n",
    "    for state in us.states.STATES:\n",
    "        geo={'for':'county:*',\n",
    "             'in':'state:{fips}'.format(fips=state.fips)}\n",
    "        for county in c.sf1.get(variables, geo=geo):\n",
    "            yield county\n",
    "\n",
    "            \n",
    "def tracts(variables='NAME'):\n",
    "    for state in us.states.STATES:\n",
    "        \n",
    "        # handy to print out state to monitor progress\n",
    "        # print state.fips, state\n",
    "        counties_in_state={'for':'county:*',\n",
    "             'in':'state:{fips}'.format(fips=state.fips)}\n",
    "        \n",
    "        for county in c.sf1.get('NAME', geo=counties_in_state):\n",
    "            \n",
    "            # print county['state'], county['NAME']\n",
    "            tracts_in_county = {'for':'tract:*',\n",
    "              'in': 'state:{s_fips} county:{c_fips}'.format(s_fips=state.fips, \n",
    "                                                            c_fips=county['county'])}\n",
    "            \n",
    "            for tract in c.sf1.get(variables,geo=tracts_in_county):\n",
    "                yield tract\n",
    "\n",
    "\n",
    "def msas(variables=\"NAME\"):\n",
    "    \n",
    "     for state in us.STATES:\n",
    "        geo = {'for':'metropolitan statistical area/micropolitan statistical area:*', \n",
    "               'in':'state:{state_fips}'.format(state_fips=state.fips)\n",
    "               }\n",
    "    \n",
    "        for msa in c.sf1.get(variables, geo=geo):\n",
    "            yield msa\n",
    "            \n",
    "def block_groups(variables='NAME'):\n",
    "    # http://api.census.gov/data/2010/sf1?get=P0010001&for=block+group:*&in=state:02+county:170\n",
    "    # let's use the county generator\n",
    "    for county in counties(variables):\n",
    "        geo = {'for':'block group:*',\n",
    "               'in':'state:{state} county:{county}'.format(state=county['state'],\n",
    "                                                county=county['county'])\n",
    "               }\n",
    "        for block_group in c.sf1.get(variables, geo):\n",
    "            yield block_group\n",
    "    \n",
    "    \n",
    "def blocks(variables='NAME'):\n",
    "    # http://api.census.gov/data/2010/sf1?get=P0010001&for=block:*&in=state:02+county:290+tract:00100\n",
    "    \n",
    "    # make use of the tract generator\n",
    "    for tract in tracts(variables):\n",
    "        geo={'for':'block:*',\n",
    "             'in':'state:{state} county:{county} tract:{tract}'.format(state=tract['state'],\n",
    "                                                                       county=tract['county'],\n",
    "                                                                       tract=tract['tract'])\n",
    "             }\n",
    "        for block in c.sf1.get(variables, geo):\n",
    "            yield block\n",
    "        \n",
    "def csas(variables=\"NAME\"):\n",
    "    # http://api.census.gov/data/2010/sf1?get=P0010001&for=combined+statistical+area:*&in=state:24\n",
    "    for state in us.STATES:\n",
    "        geo = {'for':'combined statistical area:*', \n",
    "               'in':'state:{state_fips}'.format(state_fips=state.fips)\n",
    "               }\n",
    "    \n",
    "        for csa in c.sf1.get(variables, geo=geo):\n",
    "            yield csa\n",
    "\n",
    "def districts(variables=\"NAME\"):\n",
    "    # http://api.census.gov/data/2010/sf1?get=P0010001&for=congressional+district:*&in=state:24\n",
    "    for state in us.STATES:\n",
    "        geo = {'for':'congressional district:*', \n",
    "               'in':'state:{state_fips}'.format(state_fips=state.fips)\n",
    "               }\n",
    "    \n",
    "        for district in c.sf1.get(variables, geo=geo):\n",
    "            yield district    \n",
    "            \n",
    "def zip_code_tabulation_areas(variables=\"NAME\"):\n",
    "    # http://api.census.gov/data/2010/sf1?get=P0010001&for=zip+code+tabulation+area:*&in=state:02\n",
    "    for state in us.STATES:\n",
    "        geo = {'for':'zip code tabulation area:*', \n",
    "               'in':'state:{state_fips}'.format(state_fips=state.fips)\n",
    "               }\n",
    "    \n",
    "        for zip_code_tabulation_area in c.sf1.get(variables, geo=geo):\n",
    "            yield zip_code_tabulation_area           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def census_labels(prefix='P005', n0=1, n1=17, field_width=4, include_name=True, join=False):\n",
    "    \"\"\"convenience function to generate census labels\"\"\"\n",
    "    \n",
    "    label_format = \"{i:0%dd}\" % (field_width)\n",
    "    \n",
    "    variables = [prefix + label_format.format(i=i) for i in range(n0,n1+1)]\n",
    "    if include_name:\n",
    "        variables = ['NAME'] + variables\n",
    "\n",
    "    if join:\n",
    "        return \",\".join(variables)\n",
    "    else:\n",
    "        return variables\n",
    "\n",
    "def rdot_labels(other=True):\n",
    "    if other:\n",
    "        return ['White', 'Black', 'Asian', 'Hispanic', 'Other']\n",
    "    else:\n",
    "        return ['White', 'Black', 'Asian', 'Hispanic']\n",
    "    \n",
    "FINAL_LABELS = ['NAME', 'Total'] + rdot_labels() + ['p_White', 'p_Black', 'p_Asian', 'p_Hispanic', 'p_Other'] + ['entropy5', 'entropy4', 'entropy_rice', 'gini_simpson']\n",
    "    \n",
    "def convert_to_rdotmap(row):\n",
    "    \"\"\"takes the P005 variables and maps to a series with White, Black, Asian, Hispanic, Other\n",
    "    Total\"\"\"\n",
    "    return pd.Series({'Total':row['P0050001'],\n",
    "                      'White':row['P0050003'],\n",
    "                      'Black':row['P0050004'],\n",
    "                      'Asian':row['P0050006'],\n",
    "                      'Hispanic':row['P0050010'],\n",
    "                      'Other': row['P0050005'] + row['P0050007'] + row['P0050008'] + row['P0050009'],\n",
    "                      }, index=['Total', 'White', 'Black', 'Hispanic', 'Asian', 'Other'])\n",
    "\n",
    "\n",
    "def normalize(s):\n",
    "    \"\"\"take a Series and divide each item by the sum so that the new series adds up to 1.0\"\"\"\n",
    "    total = np.sum(s)\n",
    "    return s.astype('float') / total\n",
    "    \n",
    "def normalize_relabel(s):\n",
    "    \"\"\"take a Series and divide each item by the sum so that the new series adds up to 1.0\n",
    "    Also relabel the indices by adding p_ prefix\"\"\"\n",
    "    total = np.sum(s)\n",
    "    new_index = list(Series(s.index).apply(lambda x: \"p_\"+x))\n",
    "    return Series(list(s.astype('float') / total),new_index)\n",
    "\n",
    "def entropy(series):\n",
    "    \"\"\"Normalized Shannon Index\"\"\"\n",
    "    # a series in which all the entries are equal should result in normalized entropy of 1.0\n",
    "    \n",
    "    # eliminate 0s\n",
    "    series1 = series[series!=0]\n",
    "\n",
    "    # if len(series) < 2 (i.e., 0 or 1) then return 0\n",
    "    \n",
    "    if len(series1) > 1:\n",
    "        # calculate the maximum possible entropy for given length of input series\n",
    "        max_s = -np.log(1.0/len(series))\n",
    "    \n",
    "        total = float(sum(series1))\n",
    "        p = series1.astype('float')/float(total)\n",
    "        return sum(-p*np.log(p))/max_s\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def gini_simpson(s):\n",
    "    # https://en.wikipedia.org/wiki/Diversity_index#Gini.E2.80.93Simpson_index\n",
    "    s1 = normalize(s)\n",
    "    return 1-np.sum(s1*s1)\n",
    "\n",
    "def entropy_rice(series):\n",
    "    \"\"\"hard code how Rice U did calculation \"\"\"\n",
    "    # pass in a Series with \n",
    "    # 'Asian','Black','Hispanic','White','Other'\n",
    "    # http://kinder.rice.edu/uploadedFiles/Urban_Research_Center/Media/Houston%20Region%20Grows%20More%20Ethnically%20Diverse%202-13.pdf\n",
    "\n",
    "    s0 = normalize(series)\n",
    "    s_other = s0['Other']*np.log(s0['Other']) if s0['Other'] > 0 else 0.0\n",
    "    return (np.log(0.2)*entropy(series) - s_other)/np.log(0.25)\n",
    "\n",
    "def diversity(df):\n",
    "    \"\"\"Takes a df with the P005 variables and does entropy calculation\"\"\"\n",
    "    # convert populations to int\n",
    "    df[census_labels(include_name=False)] = df[census_labels(include_name=False)].astype('int')\n",
    "    df = pd.concat((df, df.apply(convert_to_rdotmap, axis=1)),axis=1)\n",
    "    df = pd.concat((df,df[rdot_labels()].apply(normalize_relabel,axis=1)), axis=1)\n",
    "    df['entropy5'] = df.apply(lambda x:entropy(x[rdot_labels()]), axis=1)\n",
    "    df['entropy4'] = df.apply(lambda x:entropy(x[rdot_labels(other=False)]), axis=1)\n",
    "    df['entropy_rice'] = df.apply(lambda x:entropy_rice(x[rdot_labels()]), axis=1)\n",
    "    df['gini_simpson'] = df.apply(lambda x:gini_simpson(x[rdot_labels()]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grab states, convert populations to int\n",
    "states_df = DataFrame(list(states(census_labels())))\n",
    "states_df = diversity(states_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states_df[FINAL_LABELS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states_df.sort_index(by='entropy5', ascending=False)[FINAL_LABELS].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = list(counties(census_labels()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counties_df = DataFrame(r)\n",
    "counties_df = diversity(counties_df)\n",
    "counties_df[FINAL_LABELS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counties_df.sort_index(by='entropy5', ascending=False)[FINAL_LABELS].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# msas\n",
    "\n",
    "r = list(msas(census_labels()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=DataFrame(r)\n",
    "df[census_labels(include_name=False)] = df[census_labels(include_name=False)].astype('int')\n",
    "\n",
    "msas_grouped = df.groupby('metropolitan statistical area/micropolitan statistical area')\n",
    "\n",
    "#df1 = msas_grouped.apply(lambda x:Series((list(x['NAME']), sum(x['P0050001'])), index=['msas','total_pop'])).sort_index(by='total_pop', ascending=False)\n",
    "df1 = msas_grouped.apply(lambda x:Series((list(x['NAME']), ), \n",
    "                                         index=['msas']))\n",
    "\n",
    "\n",
    "df2 = msas_grouped.sum()\n",
    "df3 = pd.concat((df1,df2), axis=1)\n",
    "df3['NAME'] = df3.apply(lambda x: \"; \".join(x['msas']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msas_df = diversity(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grab the ten most populous msas and sort by entropy_rice\n",
    "msas_df.sort_index(by='Total', ascending=False)[:10].sort_index(by='entropy_rice', ascending=False)[FINAL_LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing code\n",
    "\n",
    "def to_unicode(vals):\n",
    "    return [unicode(v) for v in vals]\n",
    "\n",
    "def test_msas_df(msas_df):\n",
    "\n",
    "    min_set_of_columns =  set(['Asian','Black','Hispanic', 'Other', 'Total', 'White',\n",
    "     'entropy4', 'entropy5', 'entropy_rice', 'gini_simpson','p_Asian', 'p_Black',\n",
    "     'p_Hispanic', 'p_Other','p_White'])  \n",
    "    \n",
    "    assert min_set_of_columns & set(msas_df.columns) == min_set_of_columns\n",
    "    \n",
    "    # https://www.census.gov/geo/maps-data/data/tallies/national_geo_tallies.html\n",
    "    # 366 metro areas\n",
    "    # 576 micropolitan areas\n",
    "    \n",
    "    assert len(msas_df) == 942  \n",
    "    \n",
    "    # total number of people in metro/micro areas\n",
    "    \n",
    "    assert msas_df.Total.sum() == 289261315\n",
    "    assert msas_df['White'].sum() == 180912856\n",
    "    assert msas_df['Other'].sum() == 8540181\n",
    "    \n",
    "    # list of msas in descendng order by entropy_rice \n",
    "    top_10_metros = msas_df.sort_index(by='Total', ascending=False)[:10]\n",
    "    msa_codes_in_top_10_pop_sorted_by_entropy_rice = list(top_10_metros.sort_index(by='entropy_rice', \n",
    "                                                ascending=False).index) \n",
    "    \n",
    "    print (msa_codes_in_top_10_pop_sorted_by_entropy_rice)\n",
    "    \n",
    "    assert msa_codes_in_top_10_pop_sorted_by_entropy_rice== [u'26420', u'35620', \n",
    "                                                                         u'47900', u'31100', u'19100', \n",
    "        u'33100', u'16980', u'12060', u'37980', u'14460']\n",
    "\n",
    "\n",
    "    top_10_metro = msas_df.sort_index(by='Total', ascending=False)[:10]\n",
    "    \n",
    "    list(top_10_metro.sort_index(by='entropy_rice', ascending=False)['entropy5'])\n",
    "    \n",
    "    np.testing.assert_allclose(top_10_metro.sort_index(by='entropy_rice', ascending=False)['entropy5'], \n",
    "    [0.79628076626851163, 0.80528601550164602, 0.80809418318973791, 0.7980698349711991,\n",
    "     0.75945930510650161, 0.74913610558765376, 0.73683277781032397, 0.72964862063970914,\n",
    "     0.64082509648457675, 0.55697288400004963])\n",
    "    \n",
    "    np.testing.assert_allclose(top_10_metro.sort_index(by='entropy_rice', ascending=False)['entropy_rice'],\n",
    "    [0.87361766576115552,\n",
    "     0.87272877244078051,\n",
    "     0.85931803868749834,\n",
    "     0.85508015237749468,\n",
    "     0.82169723530719896,\n",
    "     0.81953527301129059,\n",
    "     0.80589423784325431,\n",
    "     0.78602596561378812,\n",
    "     0.68611350427640316,\n",
    "     0.56978827050565117])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you are on the right track if test_msas_df doesn't complain\n",
    "test_msas_df(msas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code to save your dataframe to a CSV\n",
    "# upload the CSV to bCourses\n",
    "# uncomment to run\n",
    "# msas_df.to_csv(\"msas_2010.csv\", encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load back the CSV and test again\n",
    "# df = DataFrame.from_csv(\"msas_2010.csv\", encoding=\"UTF-8\")\n",
    "# test_msas_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_10_metros = msas_df.sort_index(by='Total', ascending=False)[:10]\n",
    "top_10_metros['City'] = top_10_metros['NAME'].apply(lambda name: name.split('-')[0])\n",
    "top_10_metros.sort(columns=['entropy_rice'], inplace=True, ascending=True)\n",
    "\n",
    "cities = pd.Series(top_10_metros['City'])\n",
    "\n",
    "diversity = pd.Series(top_10_metros['entropy_rice'])\n",
    "\n",
    "p_white = pd.Series(top_10_metros['p_White'])\n",
    "p_asian = pd.Series(top_10_metros['p_Asian'])\n",
    "p_black = pd.Series(top_10_metros['p_Black'])\n",
    "p_latino = pd.Series(top_10_metros['p_Hispanic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# y axis locations for diversity and races\n",
    "y_div = np.arange(len(cities))*2\n",
    "y_race = (np.arange(len(cities))*2)+1\n",
    "\n",
    "# diversity bars\n",
    "pDiversity = ax.barh(y_div, diversity, alpha=0.4)\n",
    "\n",
    "# stacked horizontal bars\n",
    "pWhite = ax.barh(y_race, p_white, color='b')\n",
    "pLatino = ax.barh(y_race, p_latino, color='g', left=p_white)\n",
    "pBlack = ax.barh(y_race, p_black, color='r', left=p_white+p_latino)\n",
    "pAsian = ax.barh(y_race, p_asian, color='c', left=p_white+p_latino+p_black)\n",
    "\n",
    "plt.yticks(y_race, cities)\n",
    "\n",
    "# legend foo https://stackoverflow.com/questions/4700614/how-to-put-the-legend-out-of-the-plot\n",
    "# Shink current axis's height by 10% on the bottom\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "                 box.width, box.height * 0.85])\n",
    "\n",
    "# Put a legend below current axis\n",
    "ax.legend((pWhite, pLatino, pBlack, pAsian, pDiversity), ('White', 'Latino', 'Black', 'Asian', 'Diversity'),\n",
    "          loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# If you want to save it\n",
    "#fig.savefig('diversity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
